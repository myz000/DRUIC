{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "import tensorflow as tf\n",
    "# import tensorflow.keras as keras\n",
    "# import tensorflow.keras.backend.tensorflow_backend as KTF\n",
    " \n",
    "# config = tf.ConfigProto()  \n",
    "# config.gpu_options.allow_growth=True  \n",
    "# session = tf.Session(config=config)\n",
    " \n",
    "# KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Hypers import *\n",
    "from Utils import *\n",
    "from Preprocessing import *\n",
    "from Generator import *\n",
    "from Models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIND_type = 'small'\n",
    "data_path = '/home/myz/model/recommendations/Multi_Interest_News_Recommendation/dataset/MIND'+MIND_type\n",
    "# data_path = '/data/Multi_Interest_News_Recommendation/dataset/MIND'+MIND_type\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'train_behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding_all.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict_all.pkl\")\n",
    "vertDict_file = os.path.join(data_path, \"utils\", \"vert_dict.pkl\")\n",
    "subvertDict_file = os.path.join(data_path, \"utils\", \"subvert_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'MINR.yaml')\n",
    "\n",
    "valid_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'train', r'val_behaviors.tsv')\n",
    "test_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "test_behaviors_file = os.path.join(data_path, 'valid', r'test_behaviors.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = train_news_file\n",
    "# embedding_path = '/data/pretrained_model/glove.840B.300d.txt'\n",
    "embedding_path = '/home/myz/model/recommendations/pre-trained-model/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义新闻属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_input(news,news_index,category_dict,subcategory_dict,word_dict,content_dict,entity_dict):\n",
    "    news_num=len(news)+1\n",
    "    news_title=np.zeros((news_num,MAX_TITLE),dtype='int32')\n",
    "    news_vert=np.zeros((news_num,),dtype='int32')\n",
    "    news_subvert=np.zeros((news_num,),dtype='int32')\n",
    "    news_entity = np.zeros((news_num,MAX_ENTITY),dtype='int32')\n",
    "    news_content = np.zeros((news_num,MAX_CONTENT),dtype='int32')\n",
    "    \n",
    "    for key in news:    \n",
    "        vert,subvert,title,entity,content = news[key]\n",
    "        doc_index=news_index[key]\n",
    "        \n",
    "        news_vert[doc_index]=category_dict[vert]\n",
    "        news_subvert[doc_index]=subcategory_dict[subvert]\n",
    "        \n",
    "        \n",
    "        for word_id in range(min(MAX_TITLE,len(title))):\n",
    "            news_title[doc_index,word_id]=word_dict[title[word_id]]\n",
    "        \n",
    "        for entity_id in range(min(MAX_ENTITY,len(entity))):\n",
    "            news_entity[doc_index,entity_id]=entity_dict[entity[entity_id]]\n",
    "\n",
    "        for content_id in range(min(MAX_ENTITY,len(content))):\n",
    "            if not content[content_id] in content_dict:\n",
    "                continue\n",
    "            news_content[doc_index,content_id]=content_dict[content[content_id]]\n",
    "                       \n",
    "    return news_title,news_vert,news_subvert,news_entity,news_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news,news_index,category_dict,subcategory_dict,word_dict,content_dict,entity_dict = read_news([train_news_file,test_news_file])\n",
    "news_title,news_vert,news_subvert,news_entity,news_content=get_doc_input(news,news_index,category_dict,subcategory_dict,word_dict,content_dict,entity_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些个性化部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CLICK = 50\n",
    "MAX_TITLE = 30+50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewsCnt(train_session,news_index):\n",
    "    max_cnt=0\n",
    "    news_freq_cnt=np.zeros(len(news_index))\n",
    "    for sess in train_session:\n",
    "        clicks = sess[0]\n",
    "        for c in clicks:              \n",
    "            news_freq_cnt[news_index[c]] += 1\n",
    "            max_cnt = max(max_cnt,news_freq_cnt[news_index[c]])\n",
    "    return news_freq_cnt,max_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_hir_train_generator(Sequence):\n",
    "    def __init__(self,news_scoring,clicked_news,user_id, news_id, label, batch_size,news_freq):\n",
    "        self.news_emb = news_scoring\n",
    "        self.clicked_news = clicked_news\n",
    "\n",
    "        self.user_id = user_id\n",
    "        self.doc_id = news_id\n",
    "        self.label = label\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.ImpNum = self.label.shape[0]\n",
    "        self.news_freq = news_freq\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
    "    \n",
    "    def __get_news(self,docids):\n",
    "        news_emb = self.news_emb[docids]\n",
    "\n",
    "        return news_emb\n",
    "    \n",
    "    def random_user(self,clicked_ids):\n",
    "        clicked_p = self.news_freq[clicked_ids]\n",
    "        aug_user = []\n",
    "        ratio = int(0.25*clicked_ids.shape[1])\n",
    "        for i in range(int(clicked_ids.shape[0])):\n",
    "            clicked_p[i]/=np.sum(clicked_p[i])\n",
    "            index = [np.random.choice(clicked_ids[i], p = clicked_p[i].ravel()) for aa in range(ratio)]\n",
    "            mask_matrix = []\n",
    "            for word in clicked_ids[i]:\n",
    "                if word in index:\n",
    "                    mask_matrix.append(0)\n",
    "                else:\n",
    "                    mask_matrix.append(1)\n",
    "            mask_matrix = np.asarray(mask_matrix, dtype=\"int32\")\n",
    "            augmented_input = np.multiply(clicked_ids[i], mask_matrix)\n",
    "            aug_user.append(augmented_input)\n",
    "        aug_user = np.asarray(aug_user, dtype=\"int32\")\n",
    "        return aug_user\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed> self.ImpNum:\n",
    "            ed = self.ImpNum\n",
    "        label = self.label[start:ed]\n",
    "        \n",
    "        doc_ids = self.doc_id[start:ed]\n",
    "        title= self.__get_news(doc_ids)\n",
    "        \n",
    "        user_ids = self.user_id[start:ed]\n",
    "        clicked_ids = self.clicked_news[user_ids]\n",
    "        user_title = self.__get_news(clicked_ids)\n",
    "        \n",
    "        aug_clicked_ids = self.random_user(clicked_ids)\n",
    "        aug_user_title = self.__get_news(aug_clicked_ids)\n",
    "        \n",
    "        return ([title, user_title, aug_user_title],[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_user_generator(Sequence):\n",
    "    def __init__(self, news_scoring, userids, clicked_news,batch_size):\n",
    "        self.userids = userids\n",
    "        self.news_scoring = news_scoring\n",
    "        self.clicked_news = clicked_news\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.ImpNum = self.clicked_news.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
    "\n",
    "    def __get_news(self,docids):\n",
    "        news_scoring = self.news_scoring[docids]\n",
    "        \n",
    "        return news_scoring\n",
    "              \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed> self.ImpNum:\n",
    "            ed = self.ImpNum\n",
    "        \n",
    "        userisd = self.userids[start:ed]\n",
    "        clicked_ids = self.clicked_news[userisd]\n",
    "\n",
    "        user_title = self.__get_news(clicked_ids)\n",
    "\n",
    "        return user_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCNN(Layer):\n",
    " \n",
    "    def __init__(self, dim, topic_num, **kwargs):\n",
    "        #self.Conv1D = DepthwiseConv1D(dim,3,padding='same')\n",
    "        self.dim = dim\n",
    "        self.topic=topic_num\n",
    "        self.k=3\n",
    "        self.pad_len = int((self.k-1)/2)\n",
    "        self.Den = keras.layers.Dense(self.k*self.dim,activation='relu')\n",
    "        self.pad_layer = keras.layers.ZeroPadding1D(padding=self.pad_len)\n",
    "        \n",
    "        super(MCNN, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):       \n",
    "        self.C = self.add_weight(name='C',\n",
    "                                  shape=(self.topic,self.dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        super(MCNN, self).build(input_shape)\n",
    " \n",
    "    def call(self, x):\n",
    "        #x - (?,30,dim)\n",
    "        #x=self.Conv1D(x) #(?,30,dim)\n",
    "        kc = self.Den(self.C)#(6,k*dim)\n",
    "        att = K.dot(x,K.transpose(self.C)) #(?,30,6)\n",
    "        #ki = K.dot(att,kc) #(?,30,k*dim)\n",
    "        #ki = K.reshape(ki,shape=(-1,K.int_shape(x)[1],self.k,self.dim))#(?,30,k,dim)\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        x_pad=self.pad_layer(x)\n",
    "               \n",
    "        for i in range(self.pad_len,K.int_shape(x)[1]+self.pad_len):\n",
    "            l=i-self.pad_len\n",
    "            r=i+self.pad_len+1\n",
    "            ki = K.dot(att[:,i-self.pad_len],kc) #(?,30,k*dim)\n",
    "            ki = K.reshape(ki,shape=(-1,self.k,self.dim))#(?,k,dim)\n",
    "            output.append(tf.reduce_sum(x_pad[:,l:r,:]*ki,axis=-2))\n",
    "        output = tf.stack(output,axis=1)\n",
    "            \n",
    "        return output#(?,30,dim)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_encoder(title_word_embedding_matrix):\n",
    "    title_input = Input(shape=(MAX_TITLE,), dtype='int32')\n",
    "    \n",
    "    title_word_embedding_layer = Embedding(title_word_embedding_matrix.shape[0], title_word_embedding_matrix.shape[1], weights=[title_word_embedding_matrix],trainable=True)\n",
    "    word_vecs = title_word_embedding_layer(title_input)\n",
    "    droped_vecs = Dropout(0.2)(word_vecs)\n",
    "    word_rep = Attention(15,20)([droped_vecs]*3)\n",
    "    droped_rep = Dropout(0.2)(word_rep)\n",
    "    MCNN_layer = MCNN(300,5)\n",
    "    cnn_rep = MCNN_layer(droped_rep)\n",
    "    \n",
    "    title_vec = keras.layers.Add()([droped_rep,cnn_rep])\n",
    "    title_vec = Dropout(0.2)(title_vec)\n",
    "    print(droped_rep,cnn_rep,title_vec)\n",
    "    \n",
    "    title_vec = AttentivePooling(MAX_TITLE,300)(title_vec)\n",
    "            \n",
    "    sentEncodert = Model(title_input, title_vec)\n",
    "    return sentEncodert,MCNN_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(inputs):\n",
    "    dynamic_shape = tf.shape(inputs)\n",
    "    static_shape = inputs.get_shape().as_list()\n",
    "    shape = []\n",
    "    for i, dim in enumerate(static_shape):\n",
    "        shape.append(dim if dim is not None else dynamic_shape[i])\n",
    "\n",
    "    return shape\n",
    "\n",
    "def get_Topic_Loss(C):\n",
    "    C_norm = tf.math.l2_normalize(C, axis=-1)\n",
    "    C_sim = K.dot(C_norm,K.transpose(C_norm)) #(6,6)\n",
    "    C_sim /= 0.1 \n",
    "    c_label = tf.eye(K.int_shape(C_norm)[0])\n",
    "    L_m = tf.nn.softmax_cross_entropy_with_logits(labels=c_label, logits= C_sim) \n",
    "    L_m = tf.reduce_mean(L_m)\n",
    "    return L_m\n",
    "\n",
    "def user_contrastive_loss(user,aug_user):\n",
    "    user_norm = tf.math.l2_normalize(user, axis=-1)\n",
    "    aug_user_norm = tf.math.l2_normalize(aug_user, axis=-1)\n",
    "    C_sim = K.dot(user_norm,K.transpose(aug_user_norm)) #(6,6)\n",
    "    C_sim /= 0.1 \n",
    "    print(user_norm)\n",
    "    c_label = tf.eye(get_shape(user_norm)[0])\n",
    "    L_m = tf.nn.softmax_cross_entropy_with_logits(labels=c_label, logits= C_sim) \n",
    "    L_m = tf.reduce_mean(L_m)\n",
    "    return L_m\n",
    "\n",
    "def get_user_encoder(news_encoder):\n",
    "    clicked_title_input =  Input(shape=(MAX_CLICK,MAX_TITLE,), dtype='float32')\n",
    "    clicked_news_vecs = TimeDistributed(news_encoder)(clicked_title_input)\n",
    "    clicked_news_vecs = Dropout(0.2)(clicked_news_vecs)\n",
    "    print(clicked_news_vecs)\n",
    "    user_vec = AttentivePooling(MAX_CLICK,300)(clicked_news_vecs) #(?,400)\n",
    "    model = Model(clicked_title_input,user_vec)\n",
    "    return model \n",
    "   \n",
    "def MCCM(title_word_embedding_matrix):        \n",
    "    \n",
    "    clicked_title_input =  Input(shape=(MAX_CLICK,MAX_TITLE,), dtype='float32') \n",
    "    title_inputs = Input(shape=(1+npratio,MAX_TITLE,),dtype='float32')\n",
    "    aug_clicked_title_input =  Input(shape=(MAX_CLICK,MAX_TITLE,), dtype='float32') \n",
    "    \n",
    "    news_encoder,MCNN_layer = get_doc_encoder(title_word_embedding_matrix)    \n",
    "    news_encoder.compute_output_shape = lambda x : (x[0],300)\n",
    "\n",
    "    \n",
    "    \n",
    "    user_encoder =  get_user_encoder(news_encoder)\n",
    "    \n",
    "    user_vec = user_encoder(clicked_title_input)\n",
    "    aug_user_vec = user_encoder(aug_clicked_title_input)\n",
    "    \n",
    "    title_vecs = TimeDistributed(news_encoder)(title_inputs) #(?,5,400)\n",
    "        \n",
    "    scores = keras.layers.Dot(axes=-1)([title_vecs,user_vec])\n",
    "    \n",
    "    logits = keras.layers.Activation(keras.activations.softmax,name = 'recommend')(scores)     \n",
    "\n",
    "    model = Model([title_inputs, clicked_title_input, aug_clicked_title_input],logits) # max prob_click_positive\n",
    "    \n",
    "    C = MCNN_layer.C  #(c,dim)\n",
    "    L_m = get_Topic_Loss(C)\n",
    "    \n",
    "    model.add_loss(0.1*L_m)\n",
    "    \n",
    "    L_user = user_contrastive_loss(user_vec,aug_user_vec)\n",
    "    model.add_loss(0.1*L_user)\n",
    "    \n",
    "        \n",
    "    model.compile(loss=['categorical_crossentropy'],\n",
    "                  optimizer=Adam(lr=0.00005), \n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model,news_encoder,user_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_input(news_index,session):\n",
    "    Impressions = []\n",
    "    userid = []\n",
    "    for sess_id in range(len(session)):\n",
    "        _, poss, negs = session[sess_id]\n",
    "        imp = {'labels':[],\n",
    "                'docs':[]}\n",
    "        userid.append(sess_id)\n",
    "        for i in range(len(poss)):\n",
    "            docid = news_index[poss[i]]\n",
    "            imp['docs'].append(docid)\n",
    "            imp['labels'].append(1)\n",
    "        for i in range(len(negs)):\n",
    "            docid = news_index[negs[i]]\n",
    "            imp['docs'].append(docid)\n",
    "            imp['labels'].append(0)\n",
    "        Impressions.append(imp)\n",
    "        \n",
    "    userid = np.array(userid,dtype='int32')\n",
    "    \n",
    "    return Impressions, userid,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_impressions,news_scoring,user_scoring):\n",
    "    AUC = []\n",
    "    MRR = []\n",
    "    nDCG5 = []\n",
    "    nDCG10 = []\n",
    "    for i in range(len(test_impressions)):\n",
    "        labels = test_impressions[i]['labels']\n",
    "        nids = test_impressions[i]['docs']\n",
    "\n",
    "        uv = user_scoring[i]\n",
    "\n",
    "        nvs = news_scoring[nids]\n",
    "        score = np.dot(nvs,uv)\n",
    "\n",
    "        auc = roc_auc_score(labels,score)\n",
    "        mrr = mrr_score(labels,score)\n",
    "        ndcg5 = ndcg_score(labels,score,k=5)\n",
    "        ndcg10 = ndcg_score(labels,score,k=10)\n",
    "    \n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "        \n",
    "    AUC = np.array(AUC).mean()\n",
    "    MRR = np.array(MRR).mean()\n",
    "    nDCG5 = np.array(nDCG5).mean()\n",
    "    nDCG10 = np.array(nDCG10).mean()\n",
    "\n",
    "    \n",
    "    return AUC, MRR, nDCG5, nDCG10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同模型定义不同输入新闻信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_info = np.concatenate([news_title,news_content],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_embedding_matrix, have_word = load_matrix(embedding_path,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42055, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_word_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# entity_emb_matrix = load_entity_embedding(os.path.join(data_path,\"title_entity_emb.pkl\"),entity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = 10\n",
    "filter_nums = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/myz/anaconda3/envs/reco_gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/myz/anaconda3/envs/reco_gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Tensor(\"dropout_1/cond/Merge:0\", shape=(?, 80, 300), dtype=float32) Tensor(\"mcnn/stack:0\", shape=(?, 80, 300), dtype=float32) Tensor(\"dropout_2/cond/Merge:0\", shape=(?, 80, 300), dtype=float32)\n",
      "Tensor(\"dropout_4/cond/Merge:0\", shape=(?, 50, 300), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-16-b7ffe913c892>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Tensor(\"l2_normalize_1:0\", shape=(?, 300), dtype=float32)\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:From /home/myz/anaconda3/envs/reco_gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for t in range(times):\n",
    "    for filter_num in filter_nums:\n",
    "        file = 'Result/with_abs/'\n",
    "         \n",
    "        os.makedirs(file, exist_ok=True) \n",
    "        train_session = read_train_clickhistory(news_index,data_path,'train/train_behaviors.tsv',filter_num)\n",
    "        train_user = parse_user(news_index,train_session)\n",
    "        news_freq_cnt,max_cnt = getNewsCnt(train_session,news_index)\n",
    "        train_sess, train_user_id, train_label = get_train_input(news_index,train_session)\n",
    "        train_generator = get_hir_train_generator(news_info,train_user['click'],train_user_id,train_sess,train_label,64,news_freq_cnt)\n",
    "        \n",
    "        test_session = read_test_clickhistory(news_index,data_path,'valid/test_behaviors.tsv',filter_num)\n",
    "        test_user = parse_user(news_index,test_session)\n",
    "        #test_docids, test_userids, test_labels, test_bound = get_test_input(news_index,test_session)\n",
    "        test_impressions, test_userids= get_test_input(news_index,test_session)\n",
    "        \n",
    "        \n",
    "        model,news_encoder,user_encoder, = MCCM(title_word_embedding_matrix)\n",
    "        model.fit_generator(train_generator,epochs=8,verbose=1)\n",
    "        \n",
    "        news_scoring = news_encoder.predict(news_info,verbose=1)\n",
    "        test_generator = get_user_generator(news_info,test_userids,test_user['click'],32)\n",
    "        test_user_scoring = user_encoder.predict_generator(test_generator,verbose=1)\n",
    "        AUC, MRR, nDCG5, nDCG10 = evaluate(test_impressions,news_scoring,test_user_scoring)\n",
    "            \n",
    "        with open(file+'filterHis_'+str(filter_num)+'.txt','a+') as f:\n",
    "            f.write(\"\\n-----------\\n\")    \n",
    "            f.write(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())+\"\\n\")\n",
    "            f.write(str(AUC)+\" , \"+str(MRR)+\" , \"+str(nDCG5)+\" , \"+str(nDCG10)+\"\\n\\n\")\n",
    "            f.close()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "x_pad=self.pad_layer(x)\n",
    "\n",
    "for i in range(self.pad_len,K.int_shape(x)[1]+self.pad_len):\n",
    "    l=i-self.pad_len\n",
    "    r=i+self.pad_len+1\n",
    "    ki = K.dot(att[:,i-self.pad_len],kc) #(?,30,k*dim)\n",
    "    ki = K.reshape(ki,shape=(-1,self.k,self.dim))#(?,k,dim)\n",
    "    output.append(tf.reduce_sum(x_pad[:,l:r,:]*ki,axis=-2))\n",
    "output = tf.stack(output,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3106/3106 [==============================] - 548s 176ms/step - loss: 1.4306 - acc: 0.4730\n",
      "Epoch 2/2\n",
      "3106/3106 [==============================] - 547s 176ms/step - loss: 1.4158 - acc: 0.4783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5698297278>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator,epochs= 1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6665975699186747,\n",
       " 0.3096981600349293,\n",
       " 0.34209433566421243,\n",
       " 0.4070749467819837)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65239/65239 [==============================] - 5s 80us/sample\n",
      "1964/1964 [==============================] - 24s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "news_scoring = news_encoder.predict(news_info,verbose=1)\n",
    "test_generator = get_user_generator(news_info,test_userids,test_user['click'],32)\n",
    "test_user_scoring = user_encoder.predict_generator(test_generator,verbose=1)\n",
    "AUC, MRR, nDCG5, nDCG10 = evaluate(test_impressions,news_scoring,test_user_scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.671597354584315 0.31389079485733595 0.34758913343027414 0.411763654537403\n"
     ]
    }
   ],
   "source": [
    "print(1,AUC, MRR, nDCG5, nDCG10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3106/3106 [==============================] - 545s 175ms/step - loss: 1.4022 - acc: 0.4854\n",
      "65239/65239 [==============================] - 5s 83us/sample\n",
      "1964/1964 [==============================] - 25s 13ms/step\n",
      "8 0.6725857784605199 0.315604372477426 0.3499239765400326 0.41352306271195505\n",
      "3106/3106 [==============================] - 546s 176ms/step - loss: 1.3909 - acc: 0.4893\n",
      "65239/65239 [==============================] - 5s 82us/sample\n",
      "1964/1964 [==============================] - 24s 12ms/step\n",
      "9 0.6756547220730043 0.31996609599328213 0.3550878241421927 0.4180858262976511\n",
      "1702/3106 [===============>..............] - ETA: 4:06 - loss: 1.3842 - acc: 0.4910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(8,11):\n",
    "    model.fit_generator(train_generator,epochs= 1,verbose=1)\n",
    "    news_scoring = news_encoder.predict(news_info,verbose=1)\n",
    "    test_generator = get_user_generator(news_info,test_userids,test_user['click'],32)\n",
    "    test_user_scoring = user_encoder.predict_generator(test_generator,verbose=1)\n",
    "    AUC, MRR, nDCG5, nDCG10 = evaluate(test_impressions,news_scoring,test_user_scoring)\n",
    "    print(i,AUC, MRR, nDCG5, nDCG10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.6741253268338758 0.3190704361036573 0.3538331352457174 0.4169573365737313\n"
     ]
    }
   ],
   "source": [
    "print(i,AUC, MRR, nDCG5, nDCG10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_session = read_train_clickhistory(news_index,data_path,'train/train_behaviors.tsv',filter_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = parse_user(news_index,train_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_freq_cnt,max_cnt = getNewsCnt(train_session,news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sess, train_user_id, train_label = get_train_input(news_index,train_session)\n",
    "\n",
    "train_generator = get_hir_train_generator(news_info,train_user['click'],train_user_id,train_sess,train_label,64,news_freq_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_session = read_test_clickhistory_noclk(news_index,data_path,'valid/test_behaviors.tsv')\n",
    "test_session = read_test_clickhistory(news_index,data_path,'valid/test_behaviors.tsv',filter_num)\n",
    "test_user = parse_user(news_index,test_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_impressions, test_userids= get_test_input(news_index,test_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42055, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_word_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/reco_gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/reco_gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-11-d0faa1a2a960>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Tensor(\"l2_normalize_1:0\", shape=(?, 400), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model,news_encoder,user_encoder, = MCCM(title_word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65239/65239 [==============================] - 5s 74us/sample\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ba05960ca65f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnews_scoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_user_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_userids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'click'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_user_scoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_user_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "news_scoring = news_encoder.predict(news_info,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_scoring = clicked_news[userisd]\n",
    "test_generator = get_user_generator(news_info,test_userids,test_user['click'],32)\n",
    "test_user_scoring = user_encoder.predict_generator(test_generator,verbose=1)\n",
    "evaluate(test_impressions[:100],news_scoring,test_user_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 50, 30)]          0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 50, 400)           13550101  \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 400)               80401     \n",
      "=================================================================\n",
      "Total params: 13,630,502\n",
      "Trainable params: 13,630,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "user_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_user_scoring = user_encoder.predict_generator(test_generator,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5798275737206381,\n",
       " 0.2602605904194021,\n",
       " 0.29536643040182725,\n",
       " 0.34961313791254234)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_impressions[:100],news_scoring,test_user_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/reco_gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator,epochs=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_scoring = news_encoder.predict_generator(news_generator,verbose=1)\n",
    "test_user_generator = get_hir_user_generator(news_fetcher,test_user['click'],32)\n",
    "test_user_scoring = user_encoder.predict_generator(test_user_generator,verbose=1)\n",
    "AUC, MRR, nDCG5, nDCG10 = evaluate(test_impressions,news_scoring,test_user_scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/myz/anaconda3/envs/reco_gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/myz/anaconda3/envs/reco_gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Epoch 1/3\n",
      "2601/2601 [==============================] - 433s 167ms/step - loss: 1.4339 - acc: 0.3941\n",
      "Epoch 2/3\n",
      "2601/2601 [==============================] - 428s 165ms/step - loss: 1.3516 - acc: 0.4425\n",
      "Epoch 3/3\n",
      "2601/2601 [==============================] - 425s 164ms/step - loss: 1.3236 - acc: 0.4574\n",
      "65239/65239 [==============================] - 8s 122us/sample\n",
      "31286/31286 [==============================] - 278s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "model,news_encoder,user_encoder, = MCCM(title_word_embedding_matrix)\n",
    "\n",
    "model.fit_generator(train_generator,epochs=3)\n",
    "\n",
    "news_scoring = news_encoder.predict(news_info,verbose=1)\n",
    "test_generator = get_test_generator(news_scoring,test_docids,test_userids,test_user['click'],64)\n",
    "predicted_label = inter_model.predict_generator(test_generator,verbose=1)\n",
    "\n",
    "# parse_result(predicted_label,test_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6638470862418975,\n",
       " 0.30368736446156125,\n",
       " 0.3354127607497894,\n",
       " 0.3996658266643621)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predicted_label,test_labels,test_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2601/2601 [==============================] - 426s 164ms/step - loss: 1.3046 - acc: 0.4672\n",
      "Epoch 2/2\n",
      "2601/2601 [==============================] - 427s 164ms/step - loss: 1.2898 - acc: 0.4744\n",
      "65239/65239 [==============================] - 7s 101us/sample\n",
      "31286/31286 [==============================] - 255s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator,epochs=2)\n",
    "\n",
    "news_scoring = news_encoder.predict(news_info,verbose=1)\n",
    "test_generator = get_test_generator(news_scoring,test_docids,test_userids,test_user['click'],64)\n",
    "predicted_label = inter_model.predict_generator(test_generator,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6782388971248503,\n",
       " 0.31140355177233714,\n",
       " 0.34537388579792777,\n",
       " 0.410167162906963)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predicted_label,test_labels,test_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2601/2601 [==============================] - 423s 162ms/step - loss: 1.2770 - acc: 0.4817\n",
      "Epoch 2/2\n",
      "2601/2601 [==============================] - 430s 165ms/step - loss: 1.2649 - acc: 0.4865\n",
      "65239/65239 [==============================] - 7s 100us/sample\n",
      "31286/31286 [==============================] - 267s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator,epochs=2)\n",
    "\n",
    "news_scoring = news_encoder.predict(news_info,verbose=1)\n",
    "test_generator = get_test_generator(news_scoring,test_docids,test_userids,test_user['click'],64)\n",
    "predicted_label = inter_model.predict_generator(test_generator,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6783414368082672,\n",
       " 0.3061077266362954,\n",
       " 0.3414165614339723,\n",
       " 0.4076276224695856)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predicted_label,test_labels,test_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco_gpu)",
   "language": "python",
   "name": "reco_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
